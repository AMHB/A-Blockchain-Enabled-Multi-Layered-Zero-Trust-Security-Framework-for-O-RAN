{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRH6fHDNe9de"
      },
      "source": [
        "# Notebook Overview\n",
        "\n",
        "This notebook implements the **Intelligent Threat Detection** layer from our paper  \n",
        "“A Blockchain-Enabled Multi-Layered Zero-Trust Security Framework for O-RAN.”  \n",
        "Below is a step-by-step summary of the **9** code cells:\n",
        "\n",
        "1. **Environment Check & Imports**  \n",
        "   - Import all required Python libraries (pandas, NumPy, scikit-learn, TensorFlow, LightGBM, etc.)  \n",
        "   - Print version numbers for reproducibility.\n",
        "\n",
        "2. **Mount Google Drive**  \n",
        "   - Mount your Google Drive to `/content/drive` so that data files can be read/written.\n",
        "\n",
        "3. **Data Ingest & Train/Test Split**  \n",
        "   - Load the combined TON-IoT network CSV.  \n",
        "   - Drop rows missing the `label`.  \n",
        "   - Stratified 80/20 split into `train_network.csv` and `test_network.csv`.\n",
        "\n",
        "4. **Data Preparation & Feature Selection**  \n",
        "   - Read the saved train/test CSVs.  \n",
        "   - Drop leaky/high-cardinality fields (IP, URIs, payloads).  \n",
        "   - Encode remaining categoricals and scale numeric features.  \n",
        "   - Shuffle and shard the TRAIN split into 100 “virtual IIoT devices.”\n",
        "\n",
        "5. **Neural Network Global Pre-training**  \n",
        "   - Build a 2×64-unit MLP (ReLU→ReLU→sigmoid).  \n",
        "   - Train on **ALL** training samples (80%), validate on test (20%) for 20 epochs.  \n",
        "   - Apply a 3-epoch moving average for plotting loss & accuracy (Fig 1/2).\n",
        "\n",
        "6. **Federated Transfer Learning (NeuralNetwork_TL)**  \n",
        "   - Re-split the TRAIN data into 100 shards.  \n",
        "   - Clone the global MLP, fine-tune 10 epochs on each shard.  \n",
        "   - Aggregate weights via FedAvg → new global model w₁.  \n",
        "   - Evaluate w₁ on TEST: loss, accuracy, precision, recall, F1, AUC.\n",
        "\n",
        "7. **LightGBM Transfer Learning (LightGBM_TL)**  \n",
        "   - Pre-train LightGBM on full TRAIN (100 boosting rounds).  \n",
        "   - Fine-tune each shard for 50 rounds starting from the pre-trained model.  \n",
        "   - Select the shard model with highest AUC on TEST as the final LightGBM_TL.\n",
        "\n",
        "8. **Classical ML on Shards (RF, KNN, SVM)**  \n",
        "   - Train RandomForest (100 trees, balanced), KNN, and SVM (probability=True) on each shard.  \n",
        "   - Evaluate each model on TEST and select highest-AUC instance per algorithm as RF_No_TL, KNN_No_TL, SVM_No_TL.\n",
        "\n",
        "9. **Final Evaluation & Visualization**  \n",
        "   - Compute and tabulate accuracy, precision, recall, F1, AUC for all five models (Table I).  \n",
        "   - Plot ROC curves (Fig 3) and confusion matrices for RF/KNN/SVM (Fig 4) and LightGBM/NN_TL (Fig 5).  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keYnqzN0szxF"
      },
      "source": [
        "Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zF7rJ8XQc6b"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 1 – Environment Check & Imports\n",
        "#\n",
        "# • Ensure core libraries are installed (Colab may pre-install, but\n",
        "#   in case TensorFlow is missing we explicitly install it here).\n",
        "# • Import everything needed for data prep, ML models, and metrics.\n",
        "# • Print versions to confirm environment.\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1️⃣ Install missing core packages (quietly, only installs what’s absent)\n",
        "#    - Note: if these are already present, pip will skip or upgrade harmlessly.\n",
        "!pip install -q tensorflow scikit-learn lightgbm dask seaborn pandas numpy\n",
        "\n",
        "# 2️⃣ Now import all libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import lightgbm as lgb\n",
        "import dask\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics         import (\n",
        "    roc_curve, auc, confusion_matrix, classification_report,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "from sklearn.ensemble       import RandomForestClassifier\n",
        "from sklearn.neighbors     import KNeighborsClassifier\n",
        "from sklearn.svm           import SVC\n",
        "\n",
        "# 3️⃣ Version check\n",
        "print(\"Library versions:\")\n",
        "print(f\"  pandas:       {pd.__version__}\")\n",
        "print(f\"  numpy:        {np.__version__}\")\n",
        "print(f\"  scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"  tensorflow:   {tf.__version__}\")\n",
        "print(f\"  lightgbm:     {lgb.__version__}\")\n",
        "print(f\"  dask:         {dask.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zSqod15VVBR"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 2 – Mount Google Drive at /content/drive\n",
        "#\n",
        "# 1️⃣ This makes everything in “MyDrive” available under\n",
        "#     /content/drive/MyDrive\n",
        "# 2️⃣ Then Cell 3 can read from /content/drive/MyDrive/… directly\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')    # ← mount your entire Drive at /content/drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qwlC_XGkVcJK"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 3 – Data Ingest & Train/Test Split (with sanity checks)\n",
        "#\n",
        "# 1️⃣ Load the combined CSV\n",
        "# 2️⃣ Ensure it’s non-empty and has a ‘label’ column\n",
        "# 3️⃣ Drop any missing labels\n",
        "# 4️⃣ Do a stratified 80/20 TRAIN/TEST split\n",
        "# 5️⃣ Assert both splits are non-empty before saving\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1️⃣ Load the full dataset\n",
        "path = '/content/drive/MyDrive/Colab/ORAN_Security_IWCMC_2025/train_test_network.csv'\n",
        "dataset = pd.read_csv(path)\n",
        "print(\"✅ Loaded dataset:\", path)\n",
        "print(\"  • Shape:\", dataset.shape)\n",
        "print(\"  • Columns:\", list(dataset.columns))\n",
        "print(dataset.head(), \"\\n\")\n",
        "\n",
        "# 2️⃣ Verify we have at least one row and the label column exists\n",
        "if dataset.empty:\n",
        "    raise ValueError(f\"Loaded dataset is empty! Check that '{path}' is correct.\")\n",
        "if 'label' not in dataset.columns:\n",
        "    raise ValueError(\"No 'label' column found in the dataset.\")\n",
        "\n",
        "# 3️⃣ Clean: drop rows missing the target label\n",
        "before = dataset.shape[0]\n",
        "dataset.dropna(subset=['label'], inplace=True)\n",
        "after  = dataset.shape[0]\n",
        "print(f\"  • Dropped {before-after} rows with missing labels; {after} remain.\\n\")\n",
        "\n",
        "# 4️⃣ Stratified 80/20 split\n",
        "train_df, test_df = train_test_split(\n",
        "    dataset,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=dataset['label']\n",
        ")\n",
        "print(f\"✅ TRAIN split: {train_df.shape}  |  TEST split: {test_df.shape}\")\n",
        "print(\"  • TRAIN class ratios:\", train_df['label'].value_counts(normalize=True).to_dict())\n",
        "print(\"  • TEST  class ratios:\", test_df ['label'].value_counts(normalize=True).to_dict(), \"\\n\")\n",
        "\n",
        "# 5️⃣ Sanity-check non-emptiness before saving\n",
        "if train_df.empty or test_df.empty:\n",
        "    raise ValueError(\"One of the splits is empty! Check your input data and stratification.\")\n",
        "\n",
        "# Save to CSV for downstream cells\n",
        "train_df.to_csv(\n",
        "    '/content/drive/MyDrive/Colab/ORAN_Security_IWCMC_2025/train_network.csv',\n",
        "    index=False\n",
        ")\n",
        "test_df.to_csv(\n",
        "    '/content/drive/MyDrive/Colab/ORAN_Security_IWCMC_2025/test_network.csv',\n",
        "    index=False\n",
        ")\n",
        "print(\"✅ Saved train_network.csv & test_network.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_f0-dbsrSPb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 4 – Data Preparation & Feature Selection\n",
        "#\n",
        "# 1️⃣  Load the TRAIN / TEST splits created in Cell 3 and assert non-emptiness\n",
        "# 2️⃣  Drop any rows still missing the target label\n",
        "# 3️⃣  Remove “leaky” or high-cardinality columns that shouldn’t be modeled\n",
        "# 4️⃣  Identify feature columns vs. the label\n",
        "# 5️⃣  Label-encode any remaining non-numeric features\n",
        "# 6️⃣  Standard-scale numeric features (fit on TRAIN only)\n",
        "# 7️⃣  Shuffle TRAIN and split into 100 federated shards\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# 1️⃣ Load the 80/20 splits and sanity-check\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab/ORAN_Security_IWCMC_2025/train_network.csv')\n",
        "test_df  = pd.read_csv('/content/drive/MyDrive/Colab/ORAN_Security_IWCMC_2025/test_network.csv')\n",
        "assert not train_df.empty, \"❌ train_network.csv is empty — check your Cell 3!\"\n",
        "assert not test_df .empty, \"❌ test_network.csv is empty — check your Cell 3!\"\n",
        "\n",
        "# 2️⃣ Drop any rows without a label\n",
        "train_df.dropna(subset=['label'], inplace=True)\n",
        "test_df .dropna(subset=['label'], inplace=True)\n",
        "\n",
        "# 3️⃣ Remove “leaky” / high-cardinality columns\n",
        "drop_cols = [\n",
        "    'src_ip','src_port','dst_ip','dst_port',\n",
        "    'dns_query','http_uri','http_user_agent',\n",
        "    'http_orig_mime_types','http_resp_mime_types',\n",
        "    'ssl_subject','ssl_issuer',\n",
        "    'weird_name','weird_addl','weird_notice','type'\n",
        "]\n",
        "train_df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
        "test_df .drop(columns=drop_cols, errors='ignore', inplace=True)\n",
        "\n",
        "# 4️⃣ Identify features vs. label\n",
        "feature_cols = train_df.columns.difference(['label'])\n",
        "y_train = train_df['label']\n",
        "y_test  = test_df ['label']\n",
        "\n",
        "# 5️⃣ Encode any remaining non-numeric (low-cardinality) columns\n",
        "encoder      = LabelEncoder()\n",
        "non_num_cols = train_df.select_dtypes(exclude=np.number).columns\n",
        "for col in non_num_cols:\n",
        "    encoder.fit(pd.concat([train_df[col], test_df[col]]).unique())\n",
        "    train_df[col] = encoder.transform(train_df[col])\n",
        "    test_df [col] = encoder.transform(test_df [col])\n",
        "\n",
        "# 6️⃣ Standard-scale numeric features (fit on TRAIN only)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(train_df[feature_cols]),\n",
        "    columns=feature_cols\n",
        ")\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    scaler.transform(test_df[feature_cols]),\n",
        "    columns=feature_cols\n",
        ")\n",
        "\n",
        "# 7️⃣ Shuffle TRAIN and split into 100 federated device shards\n",
        "train_shuffled = X_train_scaled.copy()\n",
        "train_shuffled['label'] = y_train.values\n",
        "train_shuffled = train_shuffled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "num_devices = 100\n",
        "X_splits = np.array_split(train_shuffled.drop('label', axis=1), num_devices)\n",
        "y_splits = np.array_split(train_shuffled['label'],               num_devices)\n",
        "device_data = list(zip(X_splits, y_splits))\n",
        "\n",
        "print(f\"✅ Features used: {len(feature_cols)} columns after drop\")\n",
        "print(f\"✅ Prepared {num_devices} device shards; first shard has {len(X_splits[0])} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 5 – Neural-Network Global Pre-training (Sharper Start)\n",
        "# • Pre-train on ALL of TRAIN (80 %) and validate on TEST (20 %)\n",
        "# • Two 64-unit ReLU layers → sigmoid; Adam optimizer with warm LR\n",
        "# • Starts at 1e-2 for 5 epochs, then drops to 1e-3 for the remainder\n",
        "# • Runs exactly 20 epochs, smoothing for the plots only\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# 1️⃣ Build & compile the simple 2×64 MLP\n",
        "def create_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1,  activation='sigmoid')\n",
        "    ])\n",
        "    # We will override the LR via scheduler\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-2),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model(X_train_scaled.shape[1])\n",
        "\n",
        "# 2️⃣ Learning-rate schedule: high LR for first 5 epochs, then lower\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 5:\n",
        "        return 1e-2\n",
        "    else:\n",
        "        return 1e-3\n",
        "\n",
        "lr_cb = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "# 3️⃣ Pre-train on full TRAIN, validate on TEST (no internal split)\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    callbacks=[lr_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4️⃣ Smooth 3-epoch moving average for plotting only\n",
        "def smooth(x, k=3):\n",
        "    return pd.Series(x).rolling(window=k, min_periods=1).mean()\n",
        "\n",
        "train_loss = smooth(history.history['loss'])\n",
        "test_loss  = smooth(history.history['val_loss'])\n",
        "train_acc  = smooth(history.history['accuracy'])\n",
        "test_acc   = smooth(history.history['val_accuracy'])\n",
        "\n",
        "# 5️⃣ Plot Loss & Accuracy Fig 1/Fig 2\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss, label='Train Loss (MA-3)')\n",
        "plt.plot(test_loss,  label='Test Loss (MA-3)')\n",
        "plt.title('Pre-trained Model — Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_acc, label='Train Acc. (MA-3)')\n",
        "plt.plot(test_acc,  label='Test Acc. (MA-3)')\n",
        "plt.title('Pre-trained Model — Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6R3fnwnR2VVb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9yfq7pkl126"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 6 – NeuralNetwork_TL Federated Transfer Learning (Revised)\n",
        "#\n",
        "# • Re-splits the TRAIN shard into 100 for FL\n",
        "# • Clones w₀ (`model`) → fine-tunes 10 epochs per shard\n",
        "# • FedAvg → new global w₁\n",
        "# • Evaluates w₁ on TEST (loss, acc) + AUC/F1/etc.\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*swapaxes.*deprecated.*\")\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# The “global” model you pre-trained in Cell 5:\n",
        "global_model = model\n",
        "\n",
        "# 0️⃣ Re-create 100 device shards from X_train_scaled / y_train\n",
        "num_devices = 100\n",
        "X_splits = np.array_split(X_train_scaled, num_devices)\n",
        "y_splits = np.array_split(y_train,       num_devices)\n",
        "device_data = list(zip(X_splits, y_splits))\n",
        "\n",
        "# 1️⃣–2️⃣ Clone & fine-tune per shard\n",
        "local_losses, local_accs, local_models = [], [], []\n",
        "for X_dev, y_dev in device_data:\n",
        "    m = clone_model(global_model)\n",
        "    m.set_weights(global_model.get_weights())\n",
        "    m.compile(optimizer=Adam(1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    h = m.fit(X_dev, y_dev,\n",
        "              epochs=10,\n",
        "              batch_size=32,\n",
        "              verbose=0)\n",
        "    local_losses.append(h.history['loss'][-1])\n",
        "    local_accs.append(h.history['accuracy'][-1])\n",
        "    local_models.append(m)\n",
        "\n",
        "# 3️⃣ FedAvg aggregation\n",
        "def fed_avg(models):\n",
        "    return [np.mean(ws, axis=0)\n",
        "            for ws in zip(*[m.get_weights() for m in models])]\n",
        "\n",
        "new_weights = fed_avg(local_models)\n",
        "global_model.set_weights(new_weights)\n",
        "\n",
        "# 4️⃣ Evaluate w₁ on TEST\n",
        "test_loss, test_acc = global_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "# 5️⃣ Compute additional metrics\n",
        "y_prob = global_model.predict(X_test_scaled).ravel()\n",
        "y_pred = (y_prob > 0.5).astype(int)\n",
        "prec   = precision_score(y_test, y_pred)\n",
        "rec    = recall_score(y_test, y_pred)\n",
        "f1     = f1_score(y_test, y_pred)\n",
        "auc    = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"NN Local Avg → Loss: {np.mean(local_losses):.4f}, Acc: {np.mean(local_accs):.4f}\")\n",
        "print(f\"Global NN w₁ → Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "627hsDv_X9DN"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 7 – LightGBM_TL Pre-train & Device-Level TL\n",
        "#\n",
        "# • Pre-train on reduced TRAIN (100 rounds)\n",
        "# • Fine-tune each shard 50 rounds from init model\n",
        "# • Pick the shard model with highest AUC on TEST as LightGBM_TL\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "params = {\n",
        "    'objective':'binary','metric':'binary_logloss',\n",
        "    'learning_rate':0.05,'num_leaves':31,\n",
        "    'feature_fraction':0.9,'bagging_fraction':0.8,\n",
        "    'bagging_freq':5,'verbosity':-1,'seed':42\n",
        "}\n",
        "\n",
        "# 1️⃣ Pre-train\n",
        "ds_full = lgb.Dataset(X_train_scaled, label=y_train)\n",
        "global_lgb = lgb.train(params, ds_full, num_boost_round=100)\n",
        "\n",
        "# 2️⃣ Fine-tune shards & evaluate\n",
        "best_auc, best_lgb = 0.0, None\n",
        "for X_dev, y_dev in device_data:\n",
        "    ds_dev = lgb.Dataset(X_dev, label=y_dev)\n",
        "    m_dev  = lgb.train(params, ds_dev, num_boost_round=50, init_model=global_lgb)\n",
        "    auc_i  = roc_auc_score(y_test, m_dev.predict(X_test_scaled))\n",
        "    if auc_i > best_auc:\n",
        "        best_auc, best_lgb = auc_i, m_dev\n",
        "\n",
        "print(f\"Selected LightGBM_TL with AUC = {best_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KQAHHMW0YVMt"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 8 – Classical ML (RF, KNN, SVM) Device-Level TL\n",
        "#\n",
        "# • Train RF/KNN/SVM on each of 100 shards using reduced features\n",
        "# • Evaluate each on TEST, pick highest-AUC model per algorithm\n",
        "# • Stores as global_ml for final evaluation\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "bases = {\n",
        "    'RF':  RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'SVM': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "global_ml = {}\n",
        "for name, base in bases.items():\n",
        "    best_auc, best_model = 0.0, None\n",
        "    for X_dev, y_dev in device_data:\n",
        "        m = base.__class__(**base.get_params())\n",
        "        m.fit(X_dev, y_dev)\n",
        "        auc_i = roc_auc_score(y_test, m.predict_proba(X_test_scaled)[:,1])\n",
        "        if auc_i > best_auc:\n",
        "            best_auc, best_model = auc_i, m\n",
        "    global_ml[name] = best_model\n",
        "    print(f\"Selected {name}_No_TL with AUC = {best_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# Cell 9 – Final Evaluation & Results Visualization (Revised)\n",
        "#\n",
        "# • Evaluates RF_No_TL, KNN_No_TL, SVM_No_TL, LightGBM_TL, NeuralNetwork_TL\n",
        "# • Compiles Table I and plots Fig 3 (ROC), Fig 4 & Fig 5 (CMs)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_curve, auc, confusion_matrix\n",
        ")\n",
        "\n",
        "# 1️⃣ Eval helper\n",
        "def eval_model(name, model):\n",
        "    # get probabilities\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        prob = model.predict(X_test_scaled).ravel()\n",
        "    pred = (prob > 0.5).astype(int)\n",
        "\n",
        "    # metrics\n",
        "    acc  = accuracy_score(y_test, pred)\n",
        "    prec = precision_score(y_test, pred)\n",
        "    rec  = recall_score(y_test, pred)\n",
        "    f1   = f1_score(y_test, pred)\n",
        "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
        "    roc_a = auc(fpr, tpr)\n",
        "    cm   = confusion_matrix(y_test, pred)\n",
        "\n",
        "    return {\n",
        "        'Accuracy':   acc,\n",
        "        'Precision':  prec,\n",
        "        'Recall':     rec,\n",
        "        'F1 Score':   f1,\n",
        "        'AUC':        roc_a,\n",
        "        'FPR':        fpr,\n",
        "        'TPR':        tpr,\n",
        "        'CM':         cm\n",
        "    }\n",
        "\n",
        "# 2️⃣ Build results dict\n",
        "results = {\n",
        "    'RF_No_TL':         eval_model('RF_No_TL',         global_ml['RF']),\n",
        "    'KNN_No_TL':        eval_model('KNN_No_TL',        global_ml['KNN']),\n",
        "    'SVM_No_TL':        eval_model('SVM_No_TL',        global_ml['SVM']),\n",
        "    'LightGBM_TL':      eval_model('LightGBM_TL',      best_lgb),\n",
        "    'NeuralNetwork_TL': eval_model('NeuralNetwork_TL', global_model)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results).T\n",
        "print(\"TABLE I: Model Evaluation Metrics\")\n",
        "print(df[['Accuracy','Precision','Recall','F1 Score','AUC']].round(4))\n",
        "\n",
        "# Fig 3: ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "for nm, row in df.iterrows():\n",
        "    plt.plot(row['FPR'], row['TPR'], label=f\"{nm} (AUC={row['AUC']:.4f})\")\n",
        "plt.plot([0,1],[0,1],'k--', label='random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Fig 3: ROC Curves For Evaluated Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fig 4: Confusion matrices for RF, KNN, SVM\n",
        "fig, axes = plt.subplots(1,3,figsize=(15,4))\n",
        "for ax, name in zip(axes, ['RF_No_TL','KNN_No_TL','SVM_No_TL']):\n",
        "    sns.heatmap(df.loc[name,'CM'], annot=True, fmt='d', cbar=False, ax=ax)\n",
        "    ax.set_title(f'Fig 4: {name} Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fig 5: Confusion matrices for LightGBM_TL & NeuralNetwork_TL\n",
        "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
        "for ax, name in zip(axes, ['LightGBM_TL','NeuralNetwork_TL']):\n",
        "    sns.heatmap(df.loc[name,'CM'], annot=True, fmt='d', cbar=False, ax=ax)\n",
        "    ax.set_title(f'Fig 5: {name} Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6iubNOJjLJYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyP5AfV6uyr1+yIqWLzrNLC7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}